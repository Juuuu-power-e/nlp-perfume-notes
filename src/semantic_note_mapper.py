import pandas as pd
import spacy
import numpy as np
import ast
from sentence_transformers import SentenceTransformer, util
from collections import defaultdict, Counter
import os

def get_project_root():
    return os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

# ëª¨ë¸ ì´ˆê¸°í™”

project_root = get_project_root()

dataset_path = os.path.join(project_root, "data", "dataset.xlsx")
nlp = spacy.load("en_core_web_sm")
model = SentenceTransformer("all-mpnet-base-v2")

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_excel(dataset_path)


# 1. ì˜ë¯¸ ë‹¨ìœ„(semantic chunks) ì¶”ì¶œ
def extract_chunks(text):
    doc = nlp(text)
    chunks = []

    # ëª…ì‚¬êµ¬ + í˜•ìš©ì‚¬ ë‹¨ì–´ ì¶”ì¶œ
    for chunk in doc.noun_chunks:
        chunks.append(chunk.text.lower())

    for token in doc:
        if token.pos_ == "ADJ":
            if token.text.lower() not in chunks:
                chunks.append(token.text.lower())

    return list(set(chunks))

df["chunks"] = df["description"].apply(extract_chunks)

# 2. ê° chunkì— ëŒ€í•œ ìž„ë² ë”© ìƒì„±
all_chunks = sorted({chunk for chunk_list in df["chunks"] for chunk in chunk_list})
chunk_embeddings = model.encode(all_chunks, convert_to_tensor=True)

# 3. ê° chunkì— ëŒ€í•´ ìœ ì‚¬í•œ chunk ê·¸ë£¹ ì°¾ê¸° (semantic ê·¸ë£¹í•‘)
chunk_to_similar_chunks = defaultdict(list)
similarity_threshold = 0.75

for i, query_chunk in enumerate(all_chunks):
    query_vec = chunk_embeddings[i]
    hits = util.semantic_search(query_vec, chunk_embeddings, top_k=10)[0]

    for hit in hits:
        if hit["score"] >= similarity_threshold:
            similar_chunk = all_chunks[hit["corpus_id"]]
            chunk_to_similar_chunks[query_chunk].append(similar_chunk)

# 4. ê° chunk ê·¸ë£¹ì´ ë“±ìž¥í•œ ë¬¸ìž¥ì—ì„œ ë…¸íŠ¸ ìˆ˜ì§‘
chunk_group_to_notes = defaultdict(list)

for chunk, similar_chunks in chunk_to_similar_chunks.items():
    for i, row in df.iterrows():
        if any(sim_chunk in row["chunks"] for sim_chunk in similar_chunks):
            chunk_group_to_notes[chunk].extend(row["notes"])

# 5. ê° chunk ê·¸ë£¹ì— ëŒ€í•´ ê°€ìž¥ ìžì£¼ ë“±ìž¥í•œ ë…¸íŠ¸ top-k ì €ìž¥
chunk_to_top_notes = {}

for chunk, notes in chunk_group_to_notes.items():
    counter = Counter(notes)
    top_notes = [note for note, _ in counter.most_common(3)]  # top-3
    chunk_to_top_notes[chunk] = top_notes

# 6. ì˜ˆì¸¡ í•¨ìˆ˜: ìƒˆë¡œìš´ ë¬¸ìž¥ â†’ ì˜ë¯¸ ë‹¨ìœ„ ì¶”ì¶œ â†’ ê° chunk â†’ ì˜ˆì¸¡ëœ ë…¸íŠ¸ ë³‘í•©
def predict_notes(description):
    chunks = extract_chunks(description)
    predicted_notes = []

    for chunk in chunks:
        if chunk in chunk_to_top_notes:
            predicted_notes.extend(chunk_to_top_notes[chunk])
        else:
            # ìœ ì‚¬í•œ chunkê°€ ìžˆëŠ”ì§€ ì°¾ê¸°
            chunk_vec = model.encode(chunk, convert_to_tensor=True)
            hits = util.semantic_search(chunk_vec, chunk_embeddings, top_k=3)[0]
            for hit in hits:
                sim_chunk = all_chunks[hit["corpus_id"]]
                if sim_chunk in chunk_to_top_notes and hit["score"] > similarity_threshold:
                    predicted_notes.extend(chunk_to_top_notes[sim_chunk])
                    break

    return list(set(predicted_notes))  # ì¤‘ë³µ ì œê±°

# 7. ì˜ˆì‹œ ì‹¤í–‰
example = "Cozy and smoky scent with hints of vanilla"
print("ðŸ’¡ ì˜ˆì¸¡ ê²°ê³¼:", predict_notes(example))
